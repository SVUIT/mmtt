import os
import re
import requests
from urllib.parse import urljoin

# Th∆∞ m·ª•c ch·ª©a file markdown
folder_path = "./docs"
# URL g·ªëc c·ªßa website
base_url = "https://svuit.org/mmtt/docs"
# GitHub repo details
GITHUB_REPO = os.getenv("GITHUB_REPOSITORY")  # L·∫•y repo t·ª´ GitHub Actions
GITHUB_TOKEN = os.getenv("ISSUE_API")  # Token ƒë·ªÉ t·∫°o issue

# Header gi·∫£ l·∫≠p tr√¨nh duy·ªát ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_urls_from_folder(folder_path, base_url):
    """L·∫•y danh s√°ch URL t·ª´ c√°c file markdown trong th∆∞ m·ª•c."""
    urls = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".md"):
                relative_path = os.path.relpath(root, folder_path).replace("\\", "/")
                url_path = os.path.join(relative_path, file).replace("\\", "/")
                url = f"{base_url}/{url_path}".replace(".md", ".html")
                urls.append(url)
    return urls

def extract_urls_from_markdown(file_path, base_url):
    """Tr√≠ch xu·∫•t t·∫•t c·∫£ c√°c URL t·ª´ file Markdown, gi·ªØ ƒë√∫ng ph·∫ßn anchor (#)."""
    links = []
    link_pattern = r"\[.*?\]\((.*?)\)"
    
    # L·∫•y URL g·ªëc c·ªßa file markdown hi·ªán t·∫°i
    relative_path = os.path.relpath(file_path, folder_path).replace("\\", "/")
    base_file_url = f"{base_url}/{relative_path}".replace(".md", ".html")
    
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            content = file.read()
            matches = re.findall(link_pattern, content)
            for match in matches:
                if match.startswith(("http://", "https://")):
                    full_url = match  # URL tuy·ªát ƒë·ªëi
                elif match.startswith("#"):
                    full_url = base_file_url + match  # Link anchor trong file
                elif match.startswith("/"):
                    full_url = urljoin(base_url, match)  # Link tuy·ªát ƒë·ªëi theo trang g·ªëc
                else:
                    full_url = urljoin(base_file_url, match)  # Link t∆∞∆°ng ƒë·ªëi

                links.append(full_url)
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc {file_path}: {e}")
    return links

def check_url(url):
    """Ki·ªÉm tra xem URL c√≥ ho·∫°t ƒë·ªông kh√¥ng."""
    try:
        response = requests.head(url, timeout=10, headers=HEADERS, allow_redirects=True)
        if 404 <= response.status_code <= 500:
            return False, response.status_code
        return True, response.status_code
    except requests.RequestException as e:
        return False, str(e)

def create_github_issue(broken_urls):
    """T·∫°o issue tr√™n GitHub v·ªõi danh s√°ch broken links."""
    print(f"GITHUB_REPOSITORY: {os.getenv('GITHUB_REPOSITORY')}")
    print(f"ISSUE_API: {'ƒê√£ t√¨m th·∫•y' if os.getenv('ISSUE_API') else 'Kh√¥ng t√¨m th·∫•y'}")

    if not GITHUB_TOKEN or not GITHUB_REPO:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y GITHUB_TOKEN ho·∫∑c GITHUB_REPOSITORY. B·ªè qua vi·ªác t·∫°o issue.")
        return

    issue_title = "üö® Broken Links Detected!"
    issue_body = "Danh s√°ch c√°c li√™n k·∫øt b·ªã l·ªói ƒë∆∞·ª£c ph√°t hi·ªán:\n\n"
    for error in broken_urls:
        issue_body += f"- {error}\n"

    url = f"https://api.github.com/repos/{GITHUB_REPO}/issues"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    data = {
        "title": issue_title,
        "body": issue_body,
        "labels": ["broken-links"]
    }

    response = requests.post(url, json=data, headers=headers)
    if response.status_code == 201:
        print("‚úÖ Issue ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")
    else:
        print(f"‚ùå L·ªói khi t·∫°o issue: {response.status_code} - {response.text}")

if __name__ == '__main__':
    print("üîç ƒêang thu th·∫≠p danh s√°ch URL t·ª´ th∆∞ m·ª•c...")

    folder_urls = get_urls_from_folder(folder_path, base_url)
    markdown_links = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                markdown_links.extend(extract_urls_from_markdown(file_path, base_url))

    all_urls = list(set(folder_urls + markdown_links))

    broken_urls = []
    total_count = len(all_urls)
    checked_count = 0

    print(f"üîó T·ªïng s·ªë URL c·∫ßn ki·ªÉm tra: {total_count}")

    for url in all_urls:
        checked_count += 1
        print(f"({checked_count}/{total_count}) Ki·ªÉm tra: {url} ...", end=" ")
        is_available, status = check_url(url)
        if not is_available:
            print(f"‚ùå L·ªñI ({status})")
            broken_urls.append(f"{url} ‚ûù L·ªói: {status}")
        else:
            print(f"‚úÖ Ho·∫°t ƒë·ªông ({status})")

    if broken_urls:
        create_github_issue(broken_urls)
    else:
        print("\nüéâ T·∫•t c·∫£ URL ƒë·ªÅu h·ª£p l·ªá!")
